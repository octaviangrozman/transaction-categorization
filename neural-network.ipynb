{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transaction Categorization with NN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC8P7_M_1L_e"
      },
      "source": [
        "import pandas as pd\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\r\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "NCliArgc3Rdg",
        "outputId": "a82aa5b0-6746-481b-a461-2baaeafa063c"
      },
      "source": [
        "dataset = pd.read_csv('transactions_dataset.csv')\r\n",
        "dataset.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>CategoryId</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lidl                 ))))</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Deposit Rent</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>McDonalds Banegaards ))))</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>McDonalds Banegaards ))))</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Burger Shack Horsens ))))</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Text  CategoryId\n",
              "0  Lidl                 ))))           2\n",
              "1               Deposit Rent           1\n",
              "2  McDonalds Banegaards ))))           3\n",
              "3  McDonalds Banegaards ))))           3\n",
              "4  Burger Shack Horsens ))))           3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o66sAIqf37TJ",
        "outputId": "bc3c8ba3-d7ad-40c1-c265-b5be11de0294"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAADP4HT1P38"
      },
      "source": [
        "# Splitting the dataset into the Training set and Test set\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "X = dataset.iloc[:, 0]\r\n",
        "y = dataset.iloc[:, 1]\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giDyv34zEvrL",
        "outputId": "f5466a58-afbd-4f5b-e005-244808ee0360"
      },
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((505,), (127,), (505,), (127,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3btAIm513Fs"
      },
      "source": [
        "# Create a custom standardization function\r\n",
        "def custom_standardization(input_data):\r\n",
        "  text = tf.strings.lower(input_data)\r\n",
        "  text = tf.strings.regex_replace(text, '[^a-zA-Z0-9]', ' ')\r\n",
        "  # text = [ps.stem(word) for word in text if not word in set(stopwords.words('english'))]\r\n",
        "  # text = [ps.stem(word) for word in text if not word in set(stopwords.words('danish'))]\r\n",
        "  return text\r\n",
        "\r\n",
        "# Vocabulary size and number of words in a sequence.\r\n",
        "vocab_size = 200\r\n",
        "sequence_length = 30\r\n",
        "\r\n",
        "# Use the text vectorization layer to normalize, split, and map strings to \r\n",
        "# integers. Note that the layer uses the custom standardization defined above. \r\n",
        "# Set maximum_sequence length as all samples are not of the same length.\r\n",
        "vectorize_layer = TextVectorization(\r\n",
        "    standardize=custom_standardization,\r\n",
        "    max_tokens=vocab_size,\r\n",
        "    output_mode='int',\r\n",
        "    output_sequence_length=sequence_length)\r\n",
        "\r\n",
        "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\r\n",
        "vectorize_layer.adapt(X_train.to_list())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku_-Jqx427Fj"
      },
      "source": [
        "embedding_dim=5\r\n",
        "\r\n",
        "model = tf.keras.models.Sequential([\r\n",
        "  vectorize_layer,\r\n",
        "  Embedding(vocab_size, embedding_dim, name=\"embedding\"), # Embed a 200 word vocabulary into 5 dimensions\r\n",
        "  GlobalAveragePooling1D(),\r\n",
        "  Dense(16, activation='relu'),\r\n",
        "  Dense(10)\r\n",
        "])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0xJHlx9FMdR",
        "outputId": "4a8cc115-3874-460a-ab84-e83e9c3de6e0"
      },
      "source": [
        "predictions = model(X_train.to_list()[:3]).numpy()\r\n",
        "predictions"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'list'> input: ['Cafe NJoy            ))))', 'ALDI 001             ))))', 'Loevbjerg Supermarke ))))']\n",
            "Consider rewriting this model with the Functional API.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.01182186, -0.02875652,  0.0252192 , -0.0032307 ,  0.00304107,\n",
              "         0.02126542, -0.02294359, -0.01695609,  0.00255844, -0.01028241],\n",
              "       [-0.01177343, -0.02928187,  0.02556214, -0.00449242,  0.00301963,\n",
              "         0.02134806, -0.02292697, -0.01704218,  0.00263753, -0.01110735],\n",
              "       [-0.01201282, -0.02799431,  0.02609291, -0.00387859,  0.00263937,\n",
              "         0.02072632, -0.0211834 , -0.01794324,  0.00072025, -0.01075994]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g_84zgfIfND",
        "outputId": "2ae27d9b-c384-4ab6-923d-511b0a08f0fc"
      },
      "source": [
        "tf.nn.softmax(predictions).numpy()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.09922574, 0.09755953, 0.10297009, 0.10008188, 0.10071154,\n",
              "        0.10256377, 0.09812829, 0.0987176 , 0.10066295, 0.09937862],\n",
              "       [0.09925158, 0.09752896, 0.10302723, 0.09997686, 0.10073072,\n",
              "        0.10259398, 0.09815072, 0.09873001, 0.10069224, 0.09931771],\n",
              "       [0.09922385, 0.09765071, 0.10307781, 0.10003425, 0.10068841,\n",
              "        0.10252612, 0.09831807, 0.09863716, 0.10049535, 0.09934825]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrFCTLB3IF_K"
      },
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pD5EX2cIIYX",
        "outputId": "82b8dab1-be37-4b3a-ee79-22968aeffd64"
      },
      "source": [
        "loss_fn(y_train.to_list()[:3], predictions).numpy()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.2822666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuMKeqgEXNkL"
      },
      "source": [
        "model.compile(optimizer='adam',\r\n",
        "              loss=loss_fn,\r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz4rTHp3IuQ1",
        "outputId": "d7a62c4a-2e9c-4a4b-da11-8a53eadbb550"
      },
      "source": [
        "model.fit(X_train, y_train, epochs=250)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "16/16 [==============================] - 1s 2ms/step - loss: 2.2897 - accuracy: 0.2993\n",
            "Epoch 2/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 2.2561 - accuracy: 0.3098\n",
            "Epoch 3/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 2.2200 - accuracy: 0.2882\n",
            "Epoch 4/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 2.1853 - accuracy: 0.2786\n",
            "Epoch 5/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 2.1237 - accuracy: 0.3122\n",
            "Epoch 6/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 2.0753 - accuracy: 0.2829\n",
            "Epoch 7/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 2.0236 - accuracy: 0.3248\n",
            "Epoch 8/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.9665 - accuracy: 0.2959\n",
            "Epoch 9/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.9241 - accuracy: 0.2845\n",
            "Epoch 10/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.8645 - accuracy: 0.3040\n",
            "Epoch 11/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.8419 - accuracy: 0.3134\n",
            "Epoch 12/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.8515 - accuracy: 0.2741\n",
            "Epoch 13/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.8103 - accuracy: 0.2987\n",
            "Epoch 14/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.8127 - accuracy: 0.3050\n",
            "Epoch 15/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7489 - accuracy: 0.3133\n",
            "Epoch 16/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.8120 - accuracy: 0.3043\n",
            "Epoch 17/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7857 - accuracy: 0.3051\n",
            "Epoch 18/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.8402 - accuracy: 0.2907\n",
            "Epoch 19/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.8065 - accuracy: 0.3238\n",
            "Epoch 20/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7793 - accuracy: 0.3162\n",
            "Epoch 21/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7632 - accuracy: 0.2877\n",
            "Epoch 22/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7931 - accuracy: 0.2961\n",
            "Epoch 23/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7524 - accuracy: 0.2837\n",
            "Epoch 24/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7245 - accuracy: 0.3202\n",
            "Epoch 25/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7960 - accuracy: 0.2796\n",
            "Epoch 26/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7018 - accuracy: 0.3256\n",
            "Epoch 27/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7822 - accuracy: 0.3037\n",
            "Epoch 28/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.8104 - accuracy: 0.2696\n",
            "Epoch 29/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7532 - accuracy: 0.2952\n",
            "Epoch 30/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7636 - accuracy: 0.2937\n",
            "Epoch 31/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7103 - accuracy: 0.3044\n",
            "Epoch 32/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7841 - accuracy: 0.2956\n",
            "Epoch 33/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7289 - accuracy: 0.3194\n",
            "Epoch 34/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.6947 - accuracy: 0.3176\n",
            "Epoch 35/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7500 - accuracy: 0.2999\n",
            "Epoch 36/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7324 - accuracy: 0.3023\n",
            "Epoch 37/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7837 - accuracy: 0.3032\n",
            "Epoch 38/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7117 - accuracy: 0.3140\n",
            "Epoch 39/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7441 - accuracy: 0.3053\n",
            "Epoch 40/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7345 - accuracy: 0.3192\n",
            "Epoch 41/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.6932 - accuracy: 0.3176\n",
            "Epoch 42/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7318 - accuracy: 0.2796\n",
            "Epoch 43/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.6551 - accuracy: 0.3192\n",
            "Epoch 44/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7156 - accuracy: 0.3150\n",
            "Epoch 45/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7219 - accuracy: 0.2983\n",
            "Epoch 46/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.6531 - accuracy: 0.3563\n",
            "Epoch 47/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.6860 - accuracy: 0.3128\n",
            "Epoch 48/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7189 - accuracy: 0.2750\n",
            "Epoch 49/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.6792 - accuracy: 0.3078\n",
            "Epoch 50/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.6938 - accuracy: 0.3094\n",
            "Epoch 51/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.6994 - accuracy: 0.3121\n",
            "Epoch 52/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.6932 - accuracy: 0.3051\n",
            "Epoch 53/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7062 - accuracy: 0.2886\n",
            "Epoch 54/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.7222 - accuracy: 0.2628\n",
            "Epoch 55/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.6327 - accuracy: 0.3020\n",
            "Epoch 56/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.6477 - accuracy: 0.2941\n",
            "Epoch 57/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.6506 - accuracy: 0.3188\n",
            "Epoch 58/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.6564 - accuracy: 0.2893\n",
            "Epoch 59/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.6739 - accuracy: 0.3031\n",
            "Epoch 60/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.6497 - accuracy: 0.2945\n",
            "Epoch 61/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.6477 - accuracy: 0.2843\n",
            "Epoch 62/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.6381 - accuracy: 0.2778\n",
            "Epoch 63/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.5947 - accuracy: 0.3080\n",
            "Epoch 64/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.5565 - accuracy: 0.3329\n",
            "Epoch 65/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.5542 - accuracy: 0.2918\n",
            "Epoch 66/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.5754 - accuracy: 0.2978\n",
            "Epoch 67/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.5731 - accuracy: 0.2969\n",
            "Epoch 68/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.5717 - accuracy: 0.2992\n",
            "Epoch 69/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.5189 - accuracy: 0.3064\n",
            "Epoch 70/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.5329 - accuracy: 0.3309\n",
            "Epoch 71/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.5561 - accuracy: 0.3038\n",
            "Epoch 72/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.5609 - accuracy: 0.2809\n",
            "Epoch 73/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.5252 - accuracy: 0.3064\n",
            "Epoch 74/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.4519 - accuracy: 0.3243\n",
            "Epoch 75/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.5388 - accuracy: 0.2835\n",
            "Epoch 76/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.4623 - accuracy: 0.3045\n",
            "Epoch 77/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.4489 - accuracy: 0.3187\n",
            "Epoch 78/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.4487 - accuracy: 0.3220\n",
            "Epoch 79/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3992 - accuracy: 0.3562\n",
            "Epoch 80/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.4051 - accuracy: 0.3493\n",
            "Epoch 81/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.4049 - accuracy: 0.3106\n",
            "Epoch 82/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3691 - accuracy: 0.3543\n",
            "Epoch 83/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3613 - accuracy: 0.3381\n",
            "Epoch 84/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3849 - accuracy: 0.4208\n",
            "Epoch 85/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3079 - accuracy: 0.4661\n",
            "Epoch 86/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3231 - accuracy: 0.5097\n",
            "Epoch 87/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3112 - accuracy: 0.5255\n",
            "Epoch 88/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.2372 - accuracy: 0.5161\n",
            "Epoch 89/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.2896 - accuracy: 0.5276\n",
            "Epoch 90/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.2100 - accuracy: 0.5444\n",
            "Epoch 91/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1981 - accuracy: 0.5839\n",
            "Epoch 92/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.2201 - accuracy: 0.6148\n",
            "Epoch 93/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1555 - accuracy: 0.6977\n",
            "Epoch 94/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1492 - accuracy: 0.7095\n",
            "Epoch 95/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1371 - accuracy: 0.7461\n",
            "Epoch 96/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1090 - accuracy: 0.7678\n",
            "Epoch 97/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.0121 - accuracy: 0.7688\n",
            "Epoch 98/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.0553 - accuracy: 0.7741\n",
            "Epoch 99/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.0592 - accuracy: 0.7450\n",
            "Epoch 100/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.0572 - accuracy: 0.7563\n",
            "Epoch 101/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.9758 - accuracy: 0.7801\n",
            "Epoch 102/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.9944 - accuracy: 0.7735\n",
            "Epoch 103/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.9493 - accuracy: 0.7740\n",
            "Epoch 104/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.9907 - accuracy: 0.7715\n",
            "Epoch 105/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.9086 - accuracy: 0.7708\n",
            "Epoch 106/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.8537 - accuracy: 0.7953\n",
            "Epoch 107/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.8889 - accuracy: 0.7917\n",
            "Epoch 108/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.8702 - accuracy: 0.8097\n",
            "Epoch 109/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.8102 - accuracy: 0.8304\n",
            "Epoch 110/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.8471 - accuracy: 0.8130\n",
            "Epoch 111/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.8249 - accuracy: 0.8152\n",
            "Epoch 112/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7363 - accuracy: 0.8548\n",
            "Epoch 113/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7755 - accuracy: 0.8321\n",
            "Epoch 114/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7197 - accuracy: 0.8489\n",
            "Epoch 115/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7486 - accuracy: 0.8242\n",
            "Epoch 116/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7809 - accuracy: 0.8192\n",
            "Epoch 117/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7014 - accuracy: 0.8304\n",
            "Epoch 118/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6698 - accuracy: 0.8426\n",
            "Epoch 119/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7257 - accuracy: 0.8319\n",
            "Epoch 120/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7095 - accuracy: 0.8259\n",
            "Epoch 121/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6746 - accuracy: 0.8389\n",
            "Epoch 122/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6192 - accuracy: 0.8546\n",
            "Epoch 123/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6807 - accuracy: 0.8172\n",
            "Epoch 124/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6177 - accuracy: 0.8483\n",
            "Epoch 125/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6024 - accuracy: 0.8650\n",
            "Epoch 126/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6534 - accuracy: 0.8353\n",
            "Epoch 127/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6408 - accuracy: 0.8261\n",
            "Epoch 128/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5795 - accuracy: 0.8480\n",
            "Epoch 129/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6116 - accuracy: 0.8324\n",
            "Epoch 130/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5798 - accuracy: 0.8464\n",
            "Epoch 131/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5844 - accuracy: 0.8378\n",
            "Epoch 132/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5724 - accuracy: 0.8501\n",
            "Epoch 133/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5726 - accuracy: 0.8322\n",
            "Epoch 134/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6079 - accuracy: 0.8295\n",
            "Epoch 135/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5660 - accuracy: 0.8331\n",
            "Epoch 136/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5288 - accuracy: 0.8468\n",
            "Epoch 137/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5357 - accuracy: 0.8380\n",
            "Epoch 138/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5776 - accuracy: 0.8291\n",
            "Epoch 139/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5560 - accuracy: 0.8385\n",
            "Epoch 140/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5040 - accuracy: 0.8449\n",
            "Epoch 141/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5143 - accuracy: 0.8410\n",
            "Epoch 142/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5232 - accuracy: 0.8384\n",
            "Epoch 143/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5403 - accuracy: 0.8178\n",
            "Epoch 144/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4784 - accuracy: 0.8558\n",
            "Epoch 145/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4679 - accuracy: 0.8592\n",
            "Epoch 146/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5243 - accuracy: 0.8284\n",
            "Epoch 147/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4486 - accuracy: 0.8735\n",
            "Epoch 148/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4642 - accuracy: 0.8462\n",
            "Epoch 149/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4781 - accuracy: 0.8332\n",
            "Epoch 150/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4379 - accuracy: 0.8619\n",
            "Epoch 151/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4643 - accuracy: 0.8454\n",
            "Epoch 152/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4769 - accuracy: 0.8426\n",
            "Epoch 153/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4683 - accuracy: 0.8362\n",
            "Epoch 154/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4372 - accuracy: 0.8635\n",
            "Epoch 155/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4357 - accuracy: 0.8684\n",
            "Epoch 156/250\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.4450 - accuracy: 0.8462\n",
            "Epoch 157/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3774 - accuracy: 0.8791\n",
            "Epoch 158/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3518 - accuracy: 0.8849\n",
            "Epoch 159/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4263 - accuracy: 0.8561\n",
            "Epoch 160/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4410 - accuracy: 0.8483\n",
            "Epoch 161/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4221 - accuracy: 0.8639\n",
            "Epoch 162/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4347 - accuracy: 0.8334\n",
            "Epoch 163/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4241 - accuracy: 0.8411\n",
            "Epoch 164/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4405 - accuracy: 0.8562\n",
            "Epoch 165/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4304 - accuracy: 0.8464\n",
            "Epoch 166/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3912 - accuracy: 0.8523\n",
            "Epoch 167/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3682 - accuracy: 0.8744\n",
            "Epoch 168/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4468 - accuracy: 0.8483\n",
            "Epoch 169/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3937 - accuracy: 0.8536\n",
            "Epoch 170/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3592 - accuracy: 0.8636\n",
            "Epoch 171/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3889 - accuracy: 0.8627\n",
            "Epoch 172/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4150 - accuracy: 0.8563\n",
            "Epoch 173/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3866 - accuracy: 0.8680\n",
            "Epoch 174/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3485 - accuracy: 0.8719\n",
            "Epoch 175/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4129 - accuracy: 0.8519\n",
            "Epoch 176/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3991 - accuracy: 0.8669\n",
            "Epoch 177/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3448 - accuracy: 0.8838\n",
            "Epoch 178/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4078 - accuracy: 0.8448\n",
            "Epoch 179/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3472 - accuracy: 0.8937\n",
            "Epoch 180/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3756 - accuracy: 0.8537\n",
            "Epoch 181/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3718 - accuracy: 0.8794\n",
            "Epoch 182/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3661 - accuracy: 0.8712\n",
            "Epoch 183/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3645 - accuracy: 0.8802\n",
            "Epoch 184/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3487 - accuracy: 0.8788\n",
            "Epoch 185/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3361 - accuracy: 0.8790\n",
            "Epoch 186/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3639 - accuracy: 0.8763\n",
            "Epoch 187/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3045 - accuracy: 0.8965\n",
            "Epoch 188/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3208 - accuracy: 0.8912\n",
            "Epoch 189/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3054 - accuracy: 0.8894\n",
            "Epoch 190/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3416 - accuracy: 0.8871\n",
            "Epoch 191/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3358 - accuracy: 0.8679\n",
            "Epoch 192/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3312 - accuracy: 0.8823\n",
            "Epoch 193/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3628 - accuracy: 0.8592\n",
            "Epoch 194/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2900 - accuracy: 0.8985\n",
            "Epoch 195/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2888 - accuracy: 0.9049\n",
            "Epoch 196/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2836 - accuracy: 0.9038\n",
            "Epoch 197/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3072 - accuracy: 0.8830\n",
            "Epoch 198/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2913 - accuracy: 0.8818\n",
            "Epoch 199/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2972 - accuracy: 0.8897\n",
            "Epoch 200/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3199 - accuracy: 0.8847\n",
            "Epoch 201/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3442 - accuracy: 0.8782\n",
            "Epoch 202/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3036 - accuracy: 0.8745\n",
            "Epoch 203/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3220 - accuracy: 0.8667\n",
            "Epoch 204/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2829 - accuracy: 0.8973\n",
            "Epoch 205/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2503 - accuracy: 0.9031\n",
            "Epoch 206/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2777 - accuracy: 0.8892\n",
            "Epoch 207/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.3116 - accuracy: 0.8836\n",
            "Epoch 208/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2969 - accuracy: 0.8807\n",
            "Epoch 209/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2967 - accuracy: 0.8952\n",
            "Epoch 210/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2857 - accuracy: 0.9031\n",
            "Epoch 211/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2785 - accuracy: 0.9027\n",
            "Epoch 212/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2615 - accuracy: 0.9012\n",
            "Epoch 213/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2986 - accuracy: 0.8824\n",
            "Epoch 214/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2431 - accuracy: 0.8994\n",
            "Epoch 215/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.9066\n",
            "Epoch 216/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2300 - accuracy: 0.9023\n",
            "Epoch 217/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2886 - accuracy: 0.9106\n",
            "Epoch 218/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2650 - accuracy: 0.9154\n",
            "Epoch 219/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2508 - accuracy: 0.9162\n",
            "Epoch 220/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2732 - accuracy: 0.9042\n",
            "Epoch 221/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2584 - accuracy: 0.9081\n",
            "Epoch 222/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2470 - accuracy: 0.9242\n",
            "Epoch 223/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2592 - accuracy: 0.9353\n",
            "Epoch 224/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2343 - accuracy: 0.9344\n",
            "Epoch 225/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2762 - accuracy: 0.9214\n",
            "Epoch 226/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2230 - accuracy: 0.9324\n",
            "Epoch 227/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2464 - accuracy: 0.9315\n",
            "Epoch 228/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2528 - accuracy: 0.9432\n",
            "Epoch 229/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2701 - accuracy: 0.9143\n",
            "Epoch 230/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2694 - accuracy: 0.9165\n",
            "Epoch 231/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2356 - accuracy: 0.9390\n",
            "Epoch 232/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2514 - accuracy: 0.9407\n",
            "Epoch 233/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2640 - accuracy: 0.9349\n",
            "Epoch 234/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2391 - accuracy: 0.9401\n",
            "Epoch 235/250\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.2045 - accuracy: 0.9501\n",
            "Epoch 236/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2429 - accuracy: 0.9438\n",
            "Epoch 237/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2097 - accuracy: 0.9545\n",
            "Epoch 238/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2477 - accuracy: 0.9387\n",
            "Epoch 239/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2206 - accuracy: 0.9475\n",
            "Epoch 240/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2305 - accuracy: 0.9550\n",
            "Epoch 241/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2079 - accuracy: 0.9559\n",
            "Epoch 242/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2037 - accuracy: 0.9531\n",
            "Epoch 243/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2284 - accuracy: 0.9441\n",
            "Epoch 244/250\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.2286 - accuracy: 0.9422\n",
            "Epoch 245/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2527 - accuracy: 0.9221\n",
            "Epoch 246/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2100 - accuracy: 0.9475\n",
            "Epoch 247/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2390 - accuracy: 0.9496\n",
            "Epoch 248/250\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2181 - accuracy: 0.9443\n",
            "Epoch 249/250\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.2203 - accuracy: 0.9461\n",
            "Epoch 250/250\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.2032 - accuracy: 0.9565\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe57f29f940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NHNF4XBJxYH",
        "outputId": "268dd008-ea0b-4449-89ef-cfc08aafb2a4"
      },
      "source": [
        "model.evaluate(X_test,  y_test, verbose=2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 - 0s - loss: 0.3042 - accuracy: 0.9291\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.30415064096450806, 0.9291338324546814]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8ksLUnjKAHY",
        "outputId": "1f8c97a7-cc57-43fc-e442-e57067a59c66"
      },
      "source": [
        "categories = {\r\n",
        "    0:'Automobile and Transport',\r\n",
        "    1:'Housing and Real-Estate',\r\n",
        "    2:'Groceries',\r\n",
        "    3:'Recreation and Leisure',\r\n",
        "    4:'Health and Well Being',\r\n",
        "    5:'Hobby and Knowledge',\r\n",
        "    6:'Clothes and Equipment',\r\n",
        "    7:'Cash and Credit',\r\n",
        "    8:'Financial Services',\r\n",
        "    9:'Other'\r\n",
        "}\r\n",
        "\r\n",
        "def get_category_by_id(id):\r\n",
        "    return categories[id];\r\n",
        "\r\n",
        "inputs = ['lidl', 'netto ))', 'udemy', 'kfc', 'rent']\r\n",
        "predictions = model.predict_classes(inputs)\r\n",
        "{ inputs[id]: get_category_by_id(predictions[id]) for id in range(predictions.size) }"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kfc': 'Recreation and Leisure',\n",
              " 'lidl': 'Groceries',\n",
              " 'netflix': 'Other',\n",
              " 'netto ))': 'Groceries',\n",
              " 'rent': 'Housing and Real-Estate',\n",
              " 'udemy': 'Other'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsWvp90iLSO6"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}